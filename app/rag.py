# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1asUTWxM1cdASTsyr_Q4JBtMA_mHyjgYP
"""

# !pip install sentence-transformers
# !pip install transformers
# !pip install langchain
# !pip install langchain-community
# !pip install langchain_huggingface
# !pip install pymupdf
# !pip install -U bitsandbytes
# !pip install -U chromadb
# !pip install PyPDF2

import os
import torch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
import warnings
warnings.filterwarnings('ignore')

"""
RAG (Retrieval-Augmented Generation) Implementation

This module provides a question-answering system that combines document retrieval 
with language model generation. It processes PDF documents, stores their embeddings, 
and uses a language model to generate contextual responses.

Main Components:
- Document embedding using sentence-transformers
- Vector storage using Chroma
- Text generation using Hugging Face models

Usage:
    from rag import RAGService
    rag = RAGService()
    response = rag.generate_response("What is a cell?")
"""

class RAGService:
    """
    A Retrieval-Augmented Generation (RAG) service that combines document retrieval 
    with language model generation.

    This class handles:
    - Document embedding and storage
    - Similarity search in the document collection
    - Response generation using a language model

    Attributes:
        model_dir (str): Directory for saving models
        db_dir (str): Directory for vector database
        pdf_path (str): Path to the source PDF document
    """

    def __init__(self, init_mode=False):
        """
        Initialize the RAG service.

        Args:
            init_mode (bool): If True, initialize and save models. 
                            If False, load saved models.
        """
        self.model_dir = "saved_models"
        self.db_dir = "vector_db"
        self.pdf_path = "data/ConceptsofBiology-WEB.pdf"
        
        # Create directories if they don't exist
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.db_dir, exist_ok=True)
        
        login(token="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")

        if init_mode:
            self.initialize_and_save_models()
        else:
            self.load_saved_models()

    def initialize_and_save_models(self):
        """
        Initialize models and save them to disk.

        This method:
        1. Initializes the embedding model
        2. Processes and splits the PDF document
        3. Creates and saves the vector store
        4. Initializes and saves the language model and tokenizer
        """
        print("Initializing and saving models...")
        
        # Initialize embeddings
        self.embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model_name)
        
        # Process documents and create vector store
        loader = PyMuPDFLoader(self.pdf_path)
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=25)
        self.docs = text_splitter.split_documents(documents)
        
        # Save vector store
        self.vectorstore = Chroma.from_documents(
            documents=self.docs,
            embedding=self.embeddings,
            persist_directory=self.db_dir
        )
        self.vectorstore.persist()
        
        # meta-llama/Llama-3.2-1B-Instruct
        # Initialize and save LLM models
        self.tokenizer = AutoTokenizer.from_pretrained("facebook/opt-125m")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        self.model = AutoModelForCausalLM.from_pretrained("facebook/opt-125m")
        
        # Save tokenizer and model
        self.tokenizer.save_pretrained(os.path.join(self.model_dir, "tokenizer"))
        self.model.save_pretrained(os.path.join(self.model_dir, "model"))
        
        print("Models initialized and saved successfully!")

    def load_saved_models(self):
        """
        Load previously saved models from disk.

        This method loads:
        1. The embedding model
        2. The vector store
        3. The language model and tokenizer

        Raises:
            FileNotFoundError: If saved models are not found in the specified directories
        """
        print("Loading saved models...")
        
        # Load embeddings
        self.embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
        self.embeddings = HuggingFaceEmbeddings(model_name=self.embedding_model_name)
        
        # Load vector store
        self.vectorstore = Chroma(
            persist_directory=self.db_dir,
            embedding_function=self.embeddings
        )
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(os.path.join(self.model_dir, "tokenizer"))
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"
        self.model = AutoModelForCausalLM.from_pretrained(
            os.path.join(self.model_dir, "model"),
            # device_map="auto"  # Automatically choose best device (CPU/GPU)
        )
        
        print("Models loaded successfully!")

    def query_documents(self, query, k=3):
        """
        Retrieve the most relevant documents for a given query.

        Args:
            query (str): The search query text
            k (int, optional): Number of documents to retrieve. Defaults to 3.

        Returns:
            list: List of relevant document contents

        Example:
            >>> docs = rag.query_documents("What is a cell?", k=2)
        """
        results = self.vectorstore.similarity_search(query, k=k)
        return [result.page_content for result in results]

    def generate_response(self, question, context):
        """
        Generate a response using the language model based on the question and context.

        Args:
            question (str): The user's question
            context (str): The relevant context retrieved from documents

        Returns:
            str: Generated response from the language model

        Example:
            >>> context = "Cells are the basic unit of life."
            >>> response = rag.generate_response("What is a cell?", context)
        """
        prompt = f"""You are a helpful assistant, you will use the provided context to answer user questions within 200 words. 
        Read the given context before answering questions and think step by step and give a crisp and accurate answer.
        Context: {context}
        Question: {question}
        Answer:"""
        
        # prompt = f"You are a helpful assistant, you will use the provided context to answer user questions within 200 words. Read the given context before answering questions and think step by step and give a crisp and accurate answer.\nContext: {context}\nQuestion: {query}\nAnswer:"
        inputs = self.tokenizer.encode(prompt, return_tensors="pt")
        outputs = self.model.generate(inputs, max_length=300, num_return_sequences=1, temperature=0.1)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        try:
            que_ans_start = response.find("Question:",0)
            que_ans_end = response[que_ans_start+1:].find("Question:",0)
            final_res = response[que_ans_start:] if que_ans_end == -1 else response[que_ans_start:que_ans_start+1+que_ans_end]
        except:
            final_res = response
        return final_res